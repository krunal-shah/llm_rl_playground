# -*- coding: utf-8 -*-
"""Transformer Implementation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19wrYMDnXwNNYoQBLfTeznPRTCy60uCc6
"""

import torch
import torch.nn as nn
import math
from loguru import logger
import pdb
"""
Gotchas:

RMSNorm
  - learnable reweighing
  - epsilon to avoid NaN
  - rsqrt is more stable
  - final layer post-norm (optional, but works better without)

MHA
  - padding token logits to -inf
  - padded token rows go NaN because of all -inf

Init
  - embeddings need MUCH lower variance for init (1 -> 0.02)

General
  - nn.ModuleList instead of simple list, to make sure parameters are correctly
    kept track of
  - Using Linear instead of simple Matrix Multiplication (biases :))
  - math.sqrt instead of torch.sqrt(torch.tensor(dh)), messes with datatypes
    torch.tensor(dh) defaults to Long on CPU -> sqrt on integers is invalid and can also create device/dtype mismatches
"""


def log_if_nan(tensor_obj, descriptor):
    if tensor_obj.isnan().any():
        logger.info(f"{descriptor}={tensor_obj}")


class RMSNorm(nn.Module):
    def __init__(self, dim, epsilon=1e-6):
        super().__init__()
        self.dim = dim
        self.gain = nn.Parameter(torch.ones(dim))
        self.epsilon = epsilon

    # x: [B, N, d]
    def forward(self, x):
        B, N, d = x.shape
        rms = x.pow(2).mean(dim=-1, keepdim=True) + self.epsilon
        rrms = rms.rsqrt()
        return x * rrms * self.gain


class MultiHeadAttention(nn.Module):
    def __init__(self, dim, nheads, layer_i):
        super().__init__()
        self.layer_i = layer_i
        self.dim = dim
        self.nheads = nheads
        self.dim_head = int(self.dim / nheads)
        self.qkv = nn.Linear(self.dim, 3 * self.dim)
        self.proj = nn.Linear(self.dim, self.dim)

    # x: [B, N, d]
    # mask: [B, N, 1]
    def forward(self, x, mask):
        B, N, d = x.shape
        dh = self.dim_head
        h = self.nheads

        # [B, N, h, 3 * dh]
        qkv = self.qkv(x).reshape([B, N, h, 3 * dh])
        # [B, N, h, dh]
        q, k, v = qkv.chunk(3, dim=-1)

        # [B, h, N, dh]
        q = q.transpose(1, 2)
        k = k.transpose(1, 2)
        v = v.transpose(1, 2)

        # [B, h, dh, N]
        k = k.transpose(2, 3)

        # [B, h, N, N]
        qkt = q @ k
        qkt = qkt / math.sqrt(dh)
        log_if_nan(qkt, f"qkt_1_{self.layer_i}")

        # causal mask [N, N]
        causal_mask = torch.triu(torch.ones([N, N], device=x.device, dtype=torch.bool), diagonal=1)
        # query padding mask [B, 1, N, 1]
        query_padding_mask = (mask == 0).unsqueeze(1)
        combined_mask = causal_mask | query_padding_mask

        qkt = qkt.masked_fill(combined_mask, float("-inf"))
        # [B, 1, N, N]
        logits = torch.softmax(qkt, dim=-1)

        # post softmax masking to handle the NaNs resulting from softmax
        logits = logits.where(~combined_mask, float(0))

        # [B, h, N, dh]
        y = logits @ v

        # [B, N, h, dh]
        y = y.transpose(1, 2)
        y = y.reshape([B, N, d])
        y = self.proj(y)
        log_if_nan(y, f"y_1_{self.layer_i}")

        return y


class TransformerBlock(nn.Module):
    def __init__(self, dim, nheads, layer_i):
        super().__init__()
        self.layer_i = layer_i
        self.dim = dim
        self.nheads = nheads
        self.rms_x = RMSNorm(self.dim)
        self.rms_y = RMSNorm(self.dim)
        self.mha = MultiHeadAttention(self.dim, self.nheads, self.layer_i)
        self.ffn = nn.Sequential(
            nn.Linear(self.dim, 4 * self.dim),
            nn.GELU(),  # or ReLU/SiLU
            nn.Linear(4 * self.dim, self.dim),
        )

    # Implementing pre-rms norm
    # x: [B, N, d]
    def forward(self, x, mask):
        B, N, d = x.shape

        norm_x = self.rms_x(x)
        y = x + self.mha(norm_x, mask)

        norm_y = self.rms_y(y)
        z = y + self.ffn(norm_y)
        if z.isnan().any():
            logger.info(f"{z=}")

        return z


class Transformer(nn.Module):
    def __init__(self, vocab_size, max_length, eos_idx):
        super().__init__()
        self.dim = 512  # hence forth referred to as `d`
        self.nheads = 8
        self.nlayers = 8
        self.max_length = max_length
        self.vocab_size = vocab_size
        self.eos_idx = eos_idx
        self.word_embeddings = nn.Embedding(self.vocab_size, self.dim, padding_idx=0)
        self.pad_idx = self.word_embeddings.padding_idx
        # Scale down embedding initialization
        self.word_embeddings.weight.data.normal_(mean=0.0, std=0.02)
        # self.inverse_word_embeddings = nn.Embedding(self.vocab_size, self.dim, padding_idx=0)
        self.position_embeddings = nn.Parameter(torch.zeros(self.max_length, self.dim))
        # Use smaller std to avoid dominating word embeddings
        self.position_embeddings.data.normal_(mean=0.0, std=0.02)
        self.transformer_blocks = nn.ModuleList(
            [TransformerBlock(dim=self.dim, nheads=self.nheads, layer_i=i) for i in range(self.nlayers)]
        )
        # self.final_norm = RMSNorm(self.dim)
        assert self.word_embeddings.padding_idx == 0

    # x: [batch_size (B), num_tokens (N)]
    def forward(self, x):
        B, N = x.shape
        indices = x.view([-1])
        mask = torch.where(indices != self.word_embeddings.padding_idx, torch.ones_like(indices), float(0))

        # Add position embeddings only up to sequence length N (might be < max_length during inference)
        # x = self.word_embeddings(x) + self.position_embeddings[:N]
        x = self.word_embeddings(x) + self.position_embeddings
        mask = mask.view([B, N, 1])

        for i in range(self.nlayers):
            x = self.transformer_blocks[i](x, mask)

        # x = self.final_norm(x)
        logits = x @ self.word_embeddings.weight.transpose(0, 1)
        # logits = x @ self.inverse_word_embeddings.weight.transpose(0, 1)

        return logits

    def generate(self, src):
        # logger.info(f"{src.shape=}")
        src_lengths = torch.count_nonzero(src != self.pad_idx, dim=-1)
        device = src.device
        batch_size = src.shape[0]

        eos_predictions = torch.tensor([False], device=device)
        lengths_maxed = torch.tensor([False], device=device)
        # logger.info(f"{src.shape}")
        while (not torch.logical_or(eos_predictions, lengths_maxed)._is_all_true()):
            # logger.info(f"{src}")
            logits = self(src)
            # logger.info(f"{logits.shape=}")
            # logger.info(f"{src_lengths=}")
            # pdb.set_trace()
            batch_indices = torch.arange(batch_size, device=logits.device)
            # logger.info(f"{src=} {src_lengths=} {logits=}")
            logits = logits[batch_indices, src_lengths - 1, :].squeeze(1)
            # logger.info(f"{logits=}")

            # logger.info(f"{logits.shape=}")
            predictions = torch.argmax(logits, dim=-1, keepdim=False)

            # logger.info(f"{predictions.shape=}")
            # logger.info(f"{src=}")
            src[batch_indices, src_lengths] = predictions
            # logger.info(f"{predictions} {src=}")
            # logger.info(f"{src.shape=}")
            eos_predictions = (predictions == self.eos_idx)
            lengths_maxed = (src_lengths == (self.max_length - 1))
            src_lengths = torch.where((~eos_predictions) & (~lengths_maxed), src_lengths + 1, src_lengths)
            # logger.info(f"{predictions=} {src=} {eos_predictions=} {src_lengths=} {lengths_maxed=}")
        # logger.info(f"{src.shape=}")
        # logger.info(f"{src=}")
        # pdb.set_trace()
        return src
