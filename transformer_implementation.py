# -*- coding: utf-8 -*-
"""Transformer Implementation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19wrYMDnXwNNYoQBLfTeznPRTCy60uCc6
"""

import torch
import torch.nn as nn
import math

"""
Gotchas:

RMSNorm
  - Learnable reweighing
  - epsilon to avoid NaN
  - rsqrt is more stable

MHA
  - padding token logits to -inf
  - padded token rows go NaN because of all -inf

General
  - nn.ModuleList instead of simple list, to make sure parameters are correctly
    kept track of
  - Using Linear instead of simple Matrix Multiplication (biases :))
  - math.sqrt instead of torch.sqrt(torch.tensor(dh)), messes with datatypes
    torch.tensor(dh) defaults to Long on CPU -> sqrt on integers is invalid and can also create device/dtype mismatches
"""


class RMSNorm(nn.Module):
    def __init__(self, dim, epsilon=1e-6):
        super().__init__()
        self.dim = dim
        self.gain = nn.Parameter(torch.ones(dim))
        self.epsilon = epsilon

    # x: [B, N, d]
    def forward(self, x):
        B, N, d = x.shape
        rms = x.pow(2).mean(dim=-1, keepdim=True) + self.epsilon
        rrms = rms.rsqrt()
        return x * rrms * self.gain


class MultiHeadAttention(nn.Module):
    def __init__(self, dim, nheads):
        super().__init__()
        self.dim = dim
        self.nheads = nheads
        self.dim_head = int(self.dim / nheads)
        self.qkv = nn.Linear(self.dim, 3 * self.dim)
        self.proj = nn.Linear(self.dim, self.dim)

    # x: [B, N, d]
    # mask: [B, N, 1]
    def forward(self, x, mask):
        B, N, d = x.shape
        dh = self.dim_head
        h = self.nheads

        # [B, N, h, 3 * dh]
        qkv = self.qkv(x).reshape([B, N, h, 3 * dh])
        # [B, N, h, dh]
        q, k, v = qkv.chunk(3, dim=-1)

        # [B, h, N, dh]
        q = q.transpose(1, 2)
        k = k.transpose(1, 2)
        v = v.transpose(1, 2)

        # [B, h, dh, N]
        k = k.transpose(2, 3)

        # [B, h, N, N]
        qkt = q @ k
        qkt = qkt / math.sqrt(dh)

        # causal mask [N, N]
        causal_mask = torch.triu(torch.ones([N, N], device=x.device, dtype=torch.bool), diagonal=1)
        # query padding mask [B, 1, N, 1]
        query_padding_mask = (mask == 0).unsqueeze(1)
        combined_mask = causal_mask | query_padding_mask

        qkt = qkt.masked_fill(combined_mask, float("-inf"))
        # [B, 1, N, N]
        logits = torch.softmax(qkt, dim=-1)

        # post softmax masking to handle the NaNs resulting from softmax
        logits = logits.where(~combined_mask, float(0))

        # [B, h, N, dh]
        y = logits @ v

        # [B, N, h, dh]
        y = y.transpose(1, 2)
        y = y.reshape([B, N, d])
        y = self.proj(y)

        return y


class TransformerBlock(nn.Module):
    def __init__(self, dim, nheads):
        super().__init__()
        self.dim = dim
        self.nheads = nheads
        self.rms_x = RMSNorm(self.dim)
        self.rms_y = RMSNorm(self.dim)
        self.mha = MultiHeadAttention(self.dim, self.nheads)
        self.ffn = nn.Sequential(
            nn.Linear(self.dim, 4 * self.dim),
            nn.GELU(),  # or ReLU/SiLU
            nn.Linear(4 * self.dim, self.dim),
        )

    # Implementing pre-rms norm
    # x: [B, N, d]
    def forward(self, x, mask):
        B, N, d = x.shape

        norm_x = self.rms_x(x)
        y = x + self.mha(norm_x, mask)

        norm_y = self.rms_y(y)
        z = y + self.ffn(norm_y)

        return z


class Transformer(nn.Module):
    def __init__(self, vocab_size):
        super().__init__()
        self.dim = 512  # hence forth referred to as `d`
        self.nheads = 8
        self.vocab_size = vocab_size
        self.word_embeddings = nn.Embedding(self.vocab_size, self.dim, padding_idx=0)
        self.nlayers = 8
        self.transformer_blocks = nn.ModuleList(
            [TransformerBlock(dim=self.dim, nheads=self.nheads) for i in range(self.nlayers)]
        )
        assert self.word_embeddings.padding_idx == 0

    # x: [batch_size (B), num_tokens (N)]
    def forward(self, x):
        B, N = x.shape
        indices = x.view([-1])
        mask = torch.where(indices != self.word_embeddings.padding_idx, torch.ones_like(indices), float(0))

        x = self.word_embeddings(x)
        mask = mask.view([B, N, 1])

        for i in range(self.nlayers):
            x = self.transformer_blocks[i](x, mask)

        logits = x @ self.word_embeddings.weight.transpose(0, 1)
        # print(logits.shape)

        return logits


# class TransformerSFT(Transformer):
#     def forward(self, x, tgt_mask):



# transformer = Transformer()

# x = torch.randint(high = 10000, size = (2, 50))
# x = torch.tensor([[1, 2, 3, 4, 0, 0], [5, 6, 0, 0, 0, 0]])

# y = transformer(x)