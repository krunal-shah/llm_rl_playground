# -*- coding: utf-8 -*-
"""Transformer Implementation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19wrYMDnXwNNYoQBLfTeznPRTCy60uCc6
"""

import torch
import torch.nn as nn
import math
from loguru import logger
"""
Gotchas:

RMSNorm
  - learnable reweighing
  - epsilon to avoid NaN
  - rsqrt is more stable
  - final layer post-norm (optional, but works better without)

MHA
  - padding token logits to -inf
  - padded token rows go NaN because of all -inf

Init
  - embeddings need MUCH lower variance for init (1 -> 0.02)

General
  - nn.ModuleList instead of simple list, to make sure parameters are correctly
    kept track of
  - Using Linear instead of simple Matrix Multiplication (biases :))
  - math.sqrt instead of torch.sqrt(torch.tensor(dh)), messes with datatypes
    torch.tensor(dh) defaults to Long on CPU -> sqrt on integers is invalid and can also create device/dtype mismatches
"""


def log_if_nan(tensor_obj, descriptor):
    if tensor_obj.isnan().any():
        logger.info(f"{descriptor}={tensor_obj}")


class RMSNorm(nn.Module):
    def __init__(self, dim, epsilon=1e-6):
        super().__init__()
        self.dim = dim
        self.gain = nn.Parameter(torch.ones(dim))
        self.epsilon = epsilon

    # x: [B, N, d]
    def forward(self, x):
        B, N, d = x.shape
        rms = x.pow(2).mean(dim=-1, keepdim=True) + self.epsilon
        rrms = rms.rsqrt()
        return x * rrms * self.gain


class MultiHeadAttention(nn.Module):
    def __init__(self, dim, nheads, layer_i):
        super().__init__()
        self.layer_i = layer_i
        self.dim = dim
        self.nheads = nheads
        self.dim_head = int(self.dim / nheads)
        self.qkv = nn.Linear(self.dim, 3 * self.dim)
        self.proj = nn.Linear(self.dim, self.dim)

    # x: [B, N, d]
    # mask: [B, N, 1]
    def forward(self, x, mask):
        B, N, d = x.shape
        dh = self.dim_head
        h = self.nheads

        # [B, N, h, 3 * dh]
        qkv = self.qkv(x).reshape([B, N, h, 3 * dh])
        # [B, N, h, dh]
        q, k, v = qkv.chunk(3, dim=-1)

        # [B, h, N, dh]
        q = q.transpose(1, 2)
        k = k.transpose(1, 2)
        v = v.transpose(1, 2)

        # [B, h, dh, N]
        k = k.transpose(2, 3)

        # [B, h, N, N]
        qkt = q @ k
        qkt = qkt / math.sqrt(dh)
        log_if_nan(qkt, f"qkt_1_{self.layer_i}")

        # causal mask [N, N]
        causal_mask = torch.triu(torch.ones([N, N], device=x.device, dtype=torch.bool), diagonal=1)
        # query padding mask [B, 1, N, 1]
        query_padding_mask = (mask == 0).unsqueeze(1)
        combined_mask = causal_mask | query_padding_mask

        qkt = qkt.masked_fill(combined_mask, float("-inf"))
        # [B, 1, N, N]
        logits = torch.softmax(qkt, dim=-1)

        # post softmax masking to handle the NaNs resulting from softmax
        logits = logits.where(~combined_mask, float(0))

        # [B, h, N, dh]
        y = logits @ v

        # [B, N, h, dh]
        y = y.transpose(1, 2)
        y = y.reshape([B, N, d])
        y = self.proj(y)
        log_if_nan(y, f"y_1_{self.layer_i}")

        return y


class TransformerBlock(nn.Module):
    def __init__(self, dim, nheads, layer_i):
        super().__init__()
        self.layer_i = layer_i
        self.dim = dim
        self.nheads = nheads
        self.rms_x = RMSNorm(self.dim)
        self.rms_y = RMSNorm(self.dim)
        self.mha = MultiHeadAttention(self.dim, self.nheads, self.layer_i)
        self.ffn = nn.Sequential(
            nn.Linear(self.dim, 4 * self.dim),
            nn.GELU(),  # or ReLU/SiLU
            nn.Linear(4 * self.dim, self.dim),
        )

    # Implementing pre-rms norm
    # x: [B, N, d]
    def forward(self, x, mask):
        B, N, d = x.shape

        norm_x = self.rms_x(x)
        y = x + self.mha(norm_x, mask)

        norm_y = self.rms_y(y)
        z = y + self.ffn(norm_y)
        if z.isnan().any():
            logger.info(f"{z=}")

        return z


class Transformer(nn.Module):
    def __init__(self, vocab_size, max_length, eos_idx):
        super().__init__()
        self.dim = 512  # hence forth referred to as `d`
        self.nheads = 16
        self.nlayers = 32
        self.max_length = max_length
        self.vocab_size = vocab_size
        self.eos_idx = eos_idx
        self.word_embeddings = nn.Embedding(self.vocab_size, self.dim, padding_idx=0)
        self.pad_idx = self.word_embeddings.padding_idx
        # Scale down embedding initialization
        self.word_embeddings.weight.data.normal_(mean=0.0, std=0.02)
        # self.inverse_word_embeddings = nn.Embedding(self.vocab_size, self.dim, padding_idx=0)
        self.position_embeddings = nn.Parameter(torch.zeros(self.max_length, self.dim))
        # Use smaller std to avoid dominating word embeddings
        self.position_embeddings.data.normal_(mean=0.0, std=0.02)
        self.transformer_blocks = nn.ModuleList(
            [TransformerBlock(dim=self.dim, nheads=self.nheads, layer_i=i) for i in range(self.nlayers)]
        )
        # self.final_norm = RMSNorm(self.dim)
        assert self.word_embeddings.padding_idx == 0

    # x: [batch_size (B), num_tokens (N)]
    def forward(self, x):
        B, N = x.shape
        indices = x.view([-1])
        mask = torch.where(indices != self.word_embeddings.padding_idx, torch.ones_like(indices), float(0))

        # Add position embeddings only up to sequence length N (might be < max_length during inference)
        # x = self.word_embeddings(x) + self.position_embeddings[:N]
        x = self.word_embeddings(x) + self.position_embeddings
        mask = mask.view([B, N, 1])

        for i in range(self.nlayers):
            x = self.transformer_blocks[i](x, mask)

        # x = self.final_norm(x)
        logits = x @ self.word_embeddings.weight.transpose(0, 1)
        # logits = x @ self.inverse_word_embeddings.weight.transpose(0, 1)

        return logits

    def generate(self, src):
        inp = src.clone()
        inp_lengths = torch.count_nonzero(inp != self.pad_idx, dim=-1)
        device = inp.device
        batch_size = inp.shape[0]

        incomplete = torch.ones([batch_size], dtype=torch.bool, device=device)
        while incomplete.any():
            logits = self(inp)
            batch_indices = torch.arange(batch_size, device=logits.device)
            logits = logits[batch_indices, inp_lengths - 1, :].squeeze(1)

            predictions = torch.where(incomplete, torch.argmax(logits, dim=-1, keepdim=False), self.eos_idx)
            inp[batch_indices, inp_lengths] = predictions

            non_eos_predictions = (predictions != self.eos_idx)
            lengths_not_maxed = (inp_lengths != (self.max_length - 1))
            incomplete = torch.logical_and(non_eos_predictions, lengths_not_maxed)
            inp_lengths = torch.where(incomplete, inp_lengths + 1, inp_lengths)

        return inp

    def generate_with_dynamic_batching(self, src):
        # src is the input tensor with only the inputs as non-pad tensors and everything else masked (pad_idx)
        inp = src.clone()
        device = inp.device
        incomplete_inp = inp.clone()
        incomplete_inp_lengths = torch.count_nonzero(incomplete_inp != self.pad_idx, dim=-1)
        incomplete_inp_indices = torch.arange(inp.shape[0], device=device)

        while incomplete_inp.shape[0]:
            logits = self(incomplete_inp)
            incomplete_inp_batch_indices = torch.arange(incomplete_inp.shape[0], device=device)
            logits = logits[incomplete_inp_batch_indices, incomplete_inp_lengths - 1, :].squeeze(1)

            predictions = torch.argmax(logits, dim=-1, keepdim=False)

            incomplete_inp[incomplete_inp_batch_indices, incomplete_inp_lengths] = predictions

            non_eos_predictions = (predictions != self.eos_idx)
            lengths_not_maxed = (incomplete_inp_lengths != (self.max_length - 1))

            incomplete = torch.logical_and(non_eos_predictions, lengths_not_maxed)
            complete = torch.logical_not(incomplete)

            complete_arg = torch.argwhere(complete)
            complete_arg_squeezed = complete_arg.squeeze(-1)
            complete_arg = complete_arg.expand(-1, incomplete_inp.shape[-1])

            incomplete_arg = torch.argwhere(incomplete)
            incomplete_arg_squeezed = incomplete_arg.squeeze(-1)
            incomplete_arg = incomplete_arg.expand(-1, incomplete_inp.shape[-1])

            complete_inp = torch.gather(input=incomplete_inp, dim=0, index=complete_arg)
            incomplete_inp = torch.gather(input=incomplete_inp, dim=0, index=incomplete_arg)

            complete_inp_indices = torch.gather(input=incomplete_inp_indices, dim=0, index=complete_arg_squeezed)
            complete_inp_indices = complete_inp_indices.unsqueeze(-1).expand(-1, complete_inp.shape[-1])
            incomplete_inp_indices = torch.gather(input=incomplete_inp_indices, dim=0, index=incomplete_arg_squeezed)

            inp.scatter_(dim=0, index=complete_inp_indices, src=complete_inp)

            incomplete_inp_lengths = torch.gather(
                input=incomplete_inp_lengths, dim=0, index=incomplete_arg_squeezed
            ) + 1

        return inp
