{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee102825",
   "metadata": {},
   "outputs": [],
   "source": [
    "from loguru import logger\n",
    "import sys\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, LinearLR, SequentialLR\n",
    "import editdistance\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from datasets import AdditionDataset\n",
    "from transformer_implementation import Transformer\n",
    "from torch.optim import Adam\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "\"\"\"\n",
    "Gotchas:\n",
    "\n",
    "Optimization:\n",
    "  - much higher LR needed when training from scratch\n",
    "  - higher batch size! (worked wonders)\n",
    "  - was on the right track with schedule\n",
    "  - gradient clipping (optional, sometimes works better without)\n",
    "\n",
    "Dataset size:\n",
    "  - Had overestimated the learnability of transformers and their sample efficiency\n",
    "\"\"\"\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "logger.remove()\n",
    "logger.add(\n",
    "    sys.stderr,\n",
    "    colorize=True,\n",
    "    format=(\n",
    "        \"<green>{time:YYYY-MM-DD HH:mm:ss.SSS}</green> | \"\n",
    "        \"<level>{level: <8}</level> | \"\n",
    "        \"<cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - \"\n",
    "        \"<level>{message}</level>\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "def generate_dataset():\n",
    "    full_dataset = AdditionDataset()\n",
    "    generator = torch.Generator().manual_seed(42)\n",
    "    train_dataset, val_dataset, test_dataset = random_split(full_dataset, [0.96, 0.02, 0.02], generator=generator)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=128)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=64)\n",
    "    test_dataloader = DataLoader(test_dataset)\n",
    "\n",
    "    return full_dataset, train_dataloader, val_dataloader, test_dataloader\n",
    "\n",
    "\n",
    "full_dataset, train_dataloader, val_dataloader, test_dataloader = generate_dataset()\n",
    "max_length = full_dataset.max_length\n",
    "\n",
    "if torch.backends.mps.is_available:\n",
    "    logger.info(\"Using MPS\")\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    logger.info(\"Using CPU\")\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "\n",
    "writer = SummaryWriter(log_dir=f\"runs/active/{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\")\n",
    "model = Transformer(vocab_size=full_dataset.vocab_size(), max_length=max_length, eos_idx=full_dataset.eos_idx)\n",
    "model = model.to(device)\n",
    "criterion = CrossEntropyLoss(ignore_index=full_dataset.pad_idx)\n",
    "optimizer = Adam(model.parameters(), lr=2e-4)\n",
    "\n",
    "scheduler1 = LinearLR(optimizer, start_factor=0.05, total_iters=30)\n",
    "scheduler2 = CosineAnnealingLR(optimizer, T_max=1400, eta_min=1e-5)\n",
    "scheduler = SequentialLR(optimizer, schedulers=[scheduler1, scheduler2], milestones=[30])\n",
    "\n",
    "# logger.level(\"INFO\")\n",
    "\n",
    "\n",
    "def compute_generation_metrics(prompts, golds, preds):\n",
    "    num_samples = len(prompts)\n",
    "    accuracy = 0\n",
    "    edit_distance = 0\n",
    "    for prompt, gold, pred in zip(prompts, golds, preds):\n",
    "        if gold == pred:\n",
    "            accuracy += 1\n",
    "        edit_distance += editdistance.eval(gold, pred)\n",
    "        logger.debug(f\"{prompt=} {gold=} {pred=}\")\n",
    "    logger.info(f\"accuracy = {accuracy/num_samples}, edit_distance = {edit_distance/num_samples}, {num_samples=}\")\n",
    "    writer.add_scalar(\"accuracy\", accuracy/num_samples, step)\n",
    "    writer.add_scalar(\"edit_distance\", edit_distance/num_samples, step)\n",
    "    writer.add_scalar(\"num_samples\", num_samples, step)\n",
    "\n",
    "\n",
    "def validate_generate(seq, src_masked):\n",
    "    input_src_lengths = torch.count_nonzero(src_masked != model.pad_idx, dim=-1)\n",
    "    pred_tensor = model.generate(src_masked)\n",
    "\n",
    "    seq = seq.tolist()\n",
    "    pred_list = pred_tensor.tolist()\n",
    "    prompts, golds, preds = [], [], []\n",
    "    for i in range(len(pred_list)):\n",
    "        prompt = full_dataset.tensor_to_text(seq[i])\n",
    "        gold = full_dataset.tensor_to_text(seq[i][input_src_lengths[i]:])\n",
    "        pred = full_dataset.tensor_to_text(pred_list[i][input_src_lengths[i]:])\n",
    "        prompts.append(prompt)\n",
    "        golds.append(gold)\n",
    "        preds.append(pred)\n",
    "    return prompts, golds, preds\n",
    "\n",
    "\n",
    "def validate(step):\n",
    "    avg_loss = 0\n",
    "    batches = 0\n",
    "    prompts, golds, preds = [], [], []\n",
    "    for seq, masked_tgt, src_masked in tqdm(val_dataloader):\n",
    "        seq = seq.to(device)\n",
    "        src_masked = src_masked.to(device)\n",
    "        logits = model(seq)\n",
    "        logits = logits.reshape([-1, logits.shape[-1]])\n",
    "        masked_tgt = masked_tgt.reshape([-1])\n",
    "        masked_tgt = masked_tgt.to(device)\n",
    "        loss = criterion(logits, masked_tgt)\n",
    "        avg_loss += loss\n",
    "\n",
    "        _prompts, _golds, _preds = validate_generate(seq, src_masked)\n",
    "        prompts += _prompts\n",
    "        golds += _golds\n",
    "        preds += _preds\n",
    "\n",
    "        batches += 1\n",
    "    compute_generation_metrics(prompts, golds, preds)\n",
    "    writer.add_scalar(\"loss/val\", avg_loss/batches, step)\n",
    "    logger.info(f\"VALIDATE = {avg_loss/batches}\")\n",
    "\n",
    "\n",
    "step = 0\n",
    "model.train()\n",
    "for epoch in range(20):\n",
    "    logger.info(f\"Epoch: {epoch}\")\n",
    "    for seq, masked_tgt, _ in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        seq = seq.to(device)\n",
    "        masked_tgt = masked_tgt.to(device)\n",
    "\n",
    "        # seq: [batch_size (B), num_tokens (N)]\n",
    "        # logits: [B, N, vocabulary size (C)]\n",
    "        logits = model(seq)\n",
    "\n",
    "        logits = logits.reshape([-1, logits.shape[-1]])\n",
    "\n",
    "        masked_tgt = masked_tgt.reshape([-1])\n",
    "\n",
    "        loss = criterion(logits, masked_tgt)\n",
    "        writer.add_scalar(\"loss/train\", loss, step)\n",
    "\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        writer.add_scalar(\"lr\", scheduler.get_last_lr()[0], step)\n",
    "        logger.info(f\"loss: {loss}\")\n",
    "        step += 1\n",
    "        if step % 50 == 0:\n",
    "            logger.info(f\"Epoch: {epoch}, Step: {step}\")\n",
    "            model.eval()\n",
    "            validate(step)\n",
    "            model.train()\n",
    "\n",
    "writer.flush()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
